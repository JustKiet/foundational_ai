{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **RIDGE REGRESSION**\n",
    "\n",
    "## **A. CONCEPT**\n",
    "\n",
    "***Ridge Regression*** is a type of linear regression that includes an addition **penality term** to help reduce overfitting, especially when there are many predictors (features) in the dataset or when multicollinearity exists.\n",
    "\n",
    "### **1. Linear Regression Formula**\n",
    "\n",
    "In standard linear regression, we try to find the line/hyperplane that best fits the data. This is done by minimizing $SS_{residuals}$ between the obeserved values and the predicted values.\n",
    "\n",
    "$Cost function = \\sum_{i=1}^n(y_i - \\hat{y_i})^2$\n",
    "\n",
    "where:\n",
    "- $y_i$: true values\n",
    "- $\\hat{y_i}$: predicted values\n",
    "\n",
    "### **2. Ridge Regression Formula**\n",
    "\n",
    "Ridge Regression modifies the linear regression cost function by adding a panlty term to prevent large coefficients. The penalty is proportional to the square of the magnitude of the coefficients, controlled by a parameter $\\lambda$ *(also known as the regularization parameter)*\n",
    "\n",
    "$Cost function = \\sum_{i=1}^n(y_i - \\hat{y_i})^2 + \\lambda \\sum_{j=1}^p \\beta_j^2$\n",
    "\n",
    "where:\n",
    "- $\\beta_j$: coefficients of the features\n",
    "- $\\lambda$: regularization strength\n",
    "\n",
    "The penalty term $\\lambda \\sum_{j=1}^p \\beta_j^2$ forces the coefficient $\\beta_j$ to be smaller. Larger values of $\\lambda$ result in stronger regularization, shrinking the coefficients more toward zero. This reduces the complexity of the model and helps prevent overfitting.\n",
    "\n",
    "The parameter $\\lambda$ is typically chosing through *cross-validation*. A larger $\\lambda$  results in greater regularization (more shrinkage of coefficients), while a smaller $\\lambda$ allows the model to resemble ordinary least squares (OLS) regression.\n",
    "\n",
    "Ridge Regression is especially useful when there are many correlated features (multicollinearity), as it helps stabilize the coefficient estimates and reduce variance in the model.\n",
    "\n",
    "### **3. Gradient Descent for Ridge Regression**\n",
    "\n",
    "To train Ridge Regression, we need to use Gradient Descent. Gradient Descent updates the model parameters iteratively to minimize the cost function:\n",
    "\n",
    "$\\beta_j \\leftarrow \\beta_j - \\alpha \\frac{\\partial J(\\beta)}{\\partial \\beta_j}$\n",
    "\n",
    "The gradient for Ridge Regression is:\n",
    "\n",
    "$\\frac{\\partial J}{\\partial \\beta_j} = -2 \\sum_{i=1}^n x_{ij}(y_i - \\hat{y_i}) + 2 \\lambda \\beta_j$\n",
    "\n",
    "where:\n",
    "- $x_{ij}$: the j-th feature of the i-th sample.\n",
    "- $\\hat{y_i} = x_i^T \\beta$: the prediction.\n",
    "- $\\alpha$: the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **B. IMPLEMENTATION**\n",
    "\n",
    "### **0. Preparing data**\n",
    "\n",
    "The dataset that is going to be used is the ***California Housing Dataset***\n",
    "\n",
    "Derived from the 1990 U.S. Census, this dataset includes various attributes for California districts, such as *median house value, median income, housing median age, total rooms, total bedrooms, population, households, latitude, and longitude.*\n",
    "\n",
    "- **Labels**: Continuous values representing the median house value (in $1000s).\n",
    "\n",
    "- **Scope**: Includes various attributes of districts in California such as median income, house age, and geographical coordinates.\n",
    "\n",
    "- **Size**: 20,640 samples, each with 9 attributes.\n",
    "\n",
    "- **Language**: N/A (numerical data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path\n",
    "DATA_PATH = './data/housing[1].csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "      <th>ocean_proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.23</td>\n",
       "      <td>37.88</td>\n",
       "      <td>41.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>8.3252</td>\n",
       "      <td>452600.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-122.22</td>\n",
       "      <td>37.86</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7099.0</td>\n",
       "      <td>1106.0</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>8.3014</td>\n",
       "      <td>358500.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-122.24</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1467.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>7.2574</td>\n",
       "      <td>352100.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1274.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>558.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>5.6431</td>\n",
       "      <td>341300.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1627.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>3.8462</td>\n",
       "      <td>342200.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0    -122.23     37.88                41.0        880.0           129.0   \n",
       "1    -122.22     37.86                21.0       7099.0          1106.0   \n",
       "2    -122.24     37.85                52.0       1467.0           190.0   \n",
       "3    -122.25     37.85                52.0       1274.0           235.0   \n",
       "4    -122.25     37.85                52.0       1627.0           280.0   \n",
       "\n",
       "   population  households  median_income  median_house_value ocean_proximity  \n",
       "0       322.0       126.0         8.3252            452600.0        NEAR BAY  \n",
       "1      2401.0      1138.0         8.3014            358500.0        NEAR BAY  \n",
       "2       496.0       177.0         7.2574            352100.0        NEAR BAY  \n",
       "3       558.0       219.0         5.6431            341300.0        NEAR BAY  \n",
       "4       565.0       259.0         3.8462            342200.0        NEAR BAY  "
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "data = pd.read_csv(DATA_PATH)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_encoded = pd.get_dummies(data['ocean_proximity'], prefix='ocean_proximity')\n",
    "data = pd.concat([data, data_encoded], axis=1)\n",
    "data = data.drop('ocean_proximity', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "(20640, 13)\n",
      "(20640,)\n"
     ]
    }
   ],
   "source": [
    "# Seperate data into features and target\n",
    "X = data.drop('median_house_value', axis=1)\n",
    "y = data['median_house_value']\n",
    "\n",
    "print(len(X.columns))\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.32783522  1.05254828  0.98214266 -0.8048191  -0.97032521 -0.9744286\n",
      "  -0.97703285  2.34476576 -0.89115574 -0.68188905 -0.01556621  2.83074203\n",
      "  -0.38446649]\n",
      " [-1.32284391  1.04318455 -0.60701891  2.0458901   1.34827594  0.86143887\n",
      "   1.66996103  2.33223796 -0.89115574 -0.68188905 -0.01556621  2.83074203\n",
      "  -0.38446649]\n",
      " [-1.33282653  1.03850269  1.85618152 -0.53574589 -0.82556097 -0.82077735\n",
      "  -0.84363692  1.7826994  -0.89115574 -0.68188905 -0.01556621  2.83074203\n",
      "  -0.38446649]\n",
      " [-1.33781784  1.03850269  1.85618152 -0.62421459 -0.71876767 -0.76602806\n",
      "  -0.73378144  0.93296751 -0.89115574 -0.68188905 -0.01556621  2.83074203\n",
      "  -0.38446649]\n",
      " [-1.33781784  1.03850269  1.85618152 -0.46240395 -0.61197437 -0.75984669\n",
      "  -0.62915718 -0.012881   -0.89115574 -0.68188905 -0.01556621  2.83074203\n",
      "  -0.38446649]]\n",
      "[2.12963148 1.31415614 1.25869341 1.16510007 1.17289952]\n"
     ]
    }
   ],
   "source": [
    "# Normalize data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "X = scaler_X.fit_transform(X)\n",
    "y = scaler_y.fit_transform(y.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "print(X[:5])\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16512, 13)\n",
      "(4128, 13)\n",
      "(16512,)\n",
      "(4128,)\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with NaN values\n",
    "# Combine X and y for training and testing datasets\n",
    "train_data = np.column_stack((X_train, y_train))\n",
    "test_data = np.column_stack((X_test, y_test))\n",
    "\n",
    "# Drop rows with NaN values from the combined train and test data\n",
    "train_data = train_data[~np.isnan(train_data).any(axis=1)]\n",
    "test_data = test_data[~np.isnan(test_data).any(axis=1)]\n",
    "\n",
    "# Separate back into X and y after dropping NaN rows\n",
    "X_train = train_data[:, :-1]\n",
    "y_train = train_data[:, -1]\n",
    "X_test = test_data[:, :-1]\n",
    "y_test = test_data[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Training model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up hyperparameters\n",
    "ALPHA = 0.01 # The learning rate (lr)\n",
    "LAMBDA = 0.5 # The regularization parameter (lambda)\n",
    "EPOCHS = 5000 # The number of iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *1.1. Add intercept column to the features (X)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          1.26764451 -1.36797628  0.34647803  0.22471827  0.21152061\n",
      "   0.7722505   0.32292363 -0.32165429 -0.89115574 -0.68188905 -0.01556621\n",
      "  -0.35326426  2.60100692]\n",
      " [ 1.          0.7036268  -0.87169852  1.61780729  0.34206536  0.59123012\n",
      "  -0.09843989  0.67079931 -0.03061993 -0.89115574 -0.68188905 -0.01556621\n",
      "  -0.35326426  2.60100692]\n",
      " [ 1.         -0.45435647 -0.45501247 -1.95780625 -0.33863945 -0.49094197\n",
      "  -0.45077809 -0.42775547  0.1503488  -0.89115574 -0.68188905 -0.01556621\n",
      "  -0.35326426  2.60100692]\n",
      " [ 1.          1.22771405 -1.37734001  0.58485227 -0.55683169 -0.40550733\n",
      "  -0.00660236 -0.37805894 -1.01494666 -0.89115574 -0.68188905 -0.01556621\n",
      "  -0.35326426  2.60100692]\n",
      " [ 1.         -0.11494758  0.53754306  1.14105882 -0.11632172 -0.25362353\n",
      "  -0.48698327 -0.31266878 -0.16658335 -0.89115574  1.46651424 -0.01556621\n",
      "  -0.35326426 -0.38446649]]\n"
     ]
    }
   ],
   "source": [
    "# Add intercept term to X_train and X_test\n",
    "X_train = np.hstack((np.ones((X_train.shape[0], 1)), X_train)) if X_train.shape[1] == 13 else X_train\n",
    "X_test = np.hstack((np.ones((X_test.shape[0], 1)), X_test)) if X_test.shape[1] == 13 else X_test\n",
    "\n",
    "print(X_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *1.2. Initialize Coefficients*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00333474 -0.00679754 -0.0111908   0.0005447  -0.01538228  0.00744551\n",
      " -0.01440408 -0.0020603   0.01690558 -0.01130915  0.0098676   0.02075594\n",
      "  0.01103969  0.01896849]\n"
     ]
    }
   ],
   "source": [
    "beta = np.random.randn(X_train.shape[1]) * 0.01\n",
    "print(beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *1.3. Implement Gradient Descent*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Cost: 0.9873005288066656, Beta: [-0.00320672 -0.007595   -0.01414565  0.00246856 -0.01230926  0.00884514\n",
      " -0.01442485 -0.00033401  0.03055097 -0.00566761]\n",
      "Epoch 100, Cost: 0.38740653196496916, Beta: [-3.86535085e-04 -8.02280082e-02 -6.82993995e-02  8.44175185e-02\n",
      "  6.90711240e-02  5.07100171e-02 -1.06209833e-01  4.16763589e-02\n",
      "  5.37523339e-01  9.94342137e-02]\n",
      "Epoch 200, Cost: 0.3732680497423016, Beta: [-0.00067174 -0.10104476 -0.08618224  0.11032794  0.07007082  0.08955498\n",
      " -0.17213288  0.0711088   0.60140827  0.09308952]\n",
      "Epoch 300, Cost: 0.36776886645619944, Beta: [-0.00075715 -0.11868728 -0.10515683  0.12124189  0.0598668   0.12302271\n",
      " -0.22029459  0.09720096  0.61513963  0.09083751]\n",
      "Epoch 400, Cost: 0.36425629487809363, Beta: [-0.00077912 -0.13593011 -0.12371669  0.12515964  0.04690765  0.15026612\n",
      " -0.25684621  0.11894188  0.62043955  0.08968024]\n",
      "Epoch 500, Cost: 0.3618061108158911, Beta: [-0.00078934 -0.15255026 -0.14157383  0.12644829  0.03338043  0.17247619\n",
      " -0.28469757  0.13700548  0.62384522  0.08860504]\n",
      "Epoch 600, Cost: 0.3600192150415405, Beta: [-0.00079643 -0.16845802 -0.15862483  0.1267929   0.02009551  0.19075687\n",
      " -0.30593912  0.15207035  0.62667353  0.08745942]\n",
      "Epoch 700, Cost: 0.3586687455527359, Beta: [-0.00080173 -0.18363441 -0.17484024  0.12677942  0.00742278  0.20596158\n",
      " -0.32217008  0.16466989  0.62922465  0.0862569 ]\n",
      "Epoch 800, Cost: 0.357616858105226, Beta: [-0.00080573 -0.19808205 -0.19022706  0.12661438 -0.0044545   0.21874122\n",
      " -0.33460817  0.17522251  0.63156564  0.08503177]\n",
      "Epoch 900, Cost: 0.3567768977834033, Beta: [-0.0008088  -0.21181351 -0.20480928  0.1263775  -0.01545073  0.22959479\n",
      " -0.34417443  0.18406225  0.63370932  0.08381277]\n",
      "Epoch 1000, Cost: 0.35609261673211207, Beta: [-0.00081124 -0.22484789 -0.21861853  0.1261045  -0.02553848  0.23890709\n",
      " -0.35156336  0.19145942  0.63565914  0.08262036]\n",
      "Epoch 1100, Cost: 0.35552624870685806, Beta: [-0.00081327 -0.23720894 -0.23168965  0.12581428 -0.03472696  0.24697654\n",
      " -0.35729812  0.19763505  0.63741998  0.08146813]\n",
      "Epoch 1200, Cost: 0.355051545956455, Beta: [-0.00081505 -0.24892341 -0.24405858  0.12551819 -0.04304867  0.25403581\n",
      " -0.36177289  0.20277142  0.63899975  0.08036458]\n",
      "Epoch 1300, Cost: 0.3546496487700194, Beta: [-0.00081671 -0.26001983 -0.25576124  0.12522354 -0.05055038  0.26026737\n",
      " -0.3652849   0.20701995  0.6404089   0.07931453]\n",
      "Epoch 1400, Cost: 0.354306588459289, Beta: [-0.00081831 -0.27052748 -0.26683283  0.12493515 -0.05728695  0.26581517\n",
      " -0.36805856  0.21050735  0.64165943  0.07832017]\n",
      "Epoch 1500, Cost: 0.3540117426211633, Beta: [-0.00081991 -0.2804757  -0.27730747  0.12465616 -0.06331701  0.27079347\n",
      " -0.37026353  0.21334034  0.64276409  0.07738187]\n",
      "Epoch 1600, Cost: 0.35375685335157314, Beta: [-0.00082153 -0.2898934  -0.28721794  0.12438854 -0.06870002  0.27529364\n",
      " -0.37202835  0.21560933  0.64373578  0.07649876]\n",
      "Epoch 1700, Cost: 0.353535384189987, Beta: [-0.00082319 -0.29880868 -0.29659555  0.12413343 -0.07349429  0.27938916\n",
      " -0.3734506   0.21739134  0.64458712  0.07566913]\n",
      "Epoch 1800, Cost: 0.35334208536694267, Beta: [-0.0008249  -0.30724866 -0.30547006  0.12389137 -0.07775571  0.28313966\n",
      " -0.37460454  0.21875233  0.64533018  0.07489075]\n",
      "Epoch 1900, Cost: 0.3531726904862537, Beta: [-0.00082666 -0.3152393  -0.3138697   0.1236625  -0.08153696  0.28659376\n",
      " -0.3755469   0.21974903  0.64597633  0.07416107]\n",
      "Epoch 2000, Cost: 0.3530236985735439, Beta: [-0.00082846 -0.32280537 -0.32182115  0.12344666 -0.08488707  0.28979151\n",
      " -0.37632113  0.22043044  0.64653609  0.07347741]\n",
      "Epoch 2100, Cost: 0.3528922133024683, Beta: [-0.0008303  -0.32997037 -0.3293496   0.12324351 -0.08785125  0.29276603\n",
      " -0.37696066  0.22083902  0.64701916  0.07283703]\n",
      "Epoch 2200, Cost: 0.35277582171906463, Beta: [-0.00083216 -0.33675657 -0.3364788   0.12305257 -0.09047086  0.29554497\n",
      " -0.37749135  0.22101168  0.64743437  0.07223722]\n",
      "Epoch 2300, Cost: 0.35267250106097686, Beta: [-0.00083405 -0.34318503 -0.34323111  0.12287331 -0.09278353  0.29815155\n",
      " -0.37793329  0.22098059  0.64778974  0.07167534]\n",
      "Epoch 2400, Cost: 0.35258054609198514, Beta: [-0.00083594 -0.34927561 -0.34962759  0.12270512 -0.09482333  0.30060538\n",
      " -0.37830221  0.22077385  0.64809249  0.07114888]\n",
      "Epoch 2500, Cost: 0.3524985117579179, Beta: [-0.00083785 -0.35504706 -0.35568807  0.12254741 -0.09662096  0.30292312\n",
      " -0.37861048  0.22041605  0.64834911  0.07065544]\n",
      "Epoch 2600, Cost: 0.352425167499537, Beta: [-0.00083974 -0.36051705 -0.36143119  0.12239958 -0.09820401  0.30511903\n",
      " -0.37886793  0.21992874  0.64856539  0.07019277]\n",
      "Epoch 2700, Cost: 0.3523594605679685, Beta: [-0.00084163 -0.3657022  -0.3668745   0.12226102 -0.09959721  0.30720534\n",
      " -0.37908241  0.21933083  0.6487465   0.06975876]\n",
      "Epoch 2800, Cost: 0.35230048637561245, Beta: [-0.00084351 -0.3706182  -0.37203451  0.12213117 -0.10082267  0.30919263\n",
      " -0.37926027  0.21863892  0.64889705  0.06935145]\n",
      "Epoch 2900, Cost: 0.3522474643971073, Beta: [-0.00084536 -0.37527979 -0.37692676  0.12200947 -0.10190009  0.31109005\n",
      " -0.37940668  0.2178676   0.64902111  0.06896898]\n",
      "Epoch 3000, Cost: 0.3521997184815053, Beta: [-0.00084718 -0.37970088 -0.38156588  0.12189542 -0.10284705  0.31290558\n",
      " -0.37952589  0.21702968  0.64912229  0.06860967]\n",
      "Epoch 3100, Cost: 0.3521566606920491, Beta: [-0.00084897 -0.38389454 -0.38596565  0.12178852 -0.10367915  0.31464618\n",
      " -0.37962142  0.21613643  0.64920377  0.06827193]\n",
      "Epoch 3200, Cost: 0.35211777798154903, Beta: [-0.00085073 -0.3878731  -0.39013903  0.1216883  -0.10441025  0.31631797\n",
      " -0.37969623  0.21519775  0.64926836  0.0679543 ]\n",
      "Epoch 3300, Cost: 0.3520826211574396, Beta: [-0.00085245 -0.39164818 -0.39409824  0.12159433 -0.10505266  0.31792629\n",
      " -0.37975283  0.21422236  0.64931852  0.06765542]\n",
      "Epoch 3400, Cost: 0.3520507957033307, Beta: [-0.00085414 -0.39523072 -0.39785482  0.12150622 -0.10561725  0.3194759\n",
      " -0.37979335  0.21321787  0.64935642  0.06737405]\n",
      "Epoch 3500, Cost: 0.35202195411169074, Beta: [-0.00085578 -0.39863105 -0.40141961  0.12142358 -0.10611365  0.32097098\n",
      " -0.37981964  0.21219101  0.64938395  0.06710902]\n",
      "Epoch 3600, Cost: 0.3519957894512121, Beta: [-0.00085738 -0.40185887 -0.40480285  0.12134605 -0.10655038  0.32241524\n",
      " -0.37983329  0.21114763  0.64940277  0.06685926]\n",
      "Epoch 3700, Cost: 0.35197202994678684, Beta: [-0.00085893 -0.40492338 -0.4080142   0.12127332 -0.10693496  0.32381198\n",
      " -0.3798357   0.21009289  0.64941433  0.06662378]\n",
      "Epoch 3800, Cost: 0.35195043439312396, Beta: [-0.00086045 -0.40783322 -0.41106279  0.12120507 -0.107274    0.32516413\n",
      " -0.3798281   0.20903128  0.64941987  0.06640165]\n",
      "Epoch 3900, Cost: 0.3519307882573292, Beta: [-0.00086191 -0.41059657 -0.4139572   0.12114101 -0.10757336  0.32647434\n",
      " -0.37981158  0.20796671  0.6494205   0.06619203]\n",
      "Epoch 4000, Cost: 0.3519129003531245, Beta: [-0.00086334 -0.41322114 -0.41670556  0.12108089 -0.10783816  0.32774495\n",
      " -0.37978711  0.2069026   0.64941715  0.06599412]\n",
      "Epoch 4100, Cost: 0.35189659999127876, Beta: [-0.00086472 -0.41571421 -0.41931555  0.12102445 -0.10807293  0.32897807\n",
      " -0.37975556  0.20584191  0.64941064  0.06580719]\n",
      "Epoch 4200, Cost: 0.3518817345283775, Beta: [-0.00086606 -0.41808266 -0.4217944   0.12097146 -0.10828164  0.33017561\n",
      " -0.3797177   0.20478721  0.64940167  0.06563055]\n",
      "Epoch 4300, Cost: 0.35186816725017755, Beta: [-0.00086735 -0.420333   -0.42414898  0.1209217  -0.10846777  0.33133929\n",
      " -0.37967423  0.2037407   0.64939084  0.06546359]\n",
      "Epoch 4400, Cost: 0.35185577553717234, Beta: [-0.0008686  -0.42247135 -0.42638575  0.12087498 -0.10863437  0.33247066\n",
      " -0.37962578  0.20270429  0.64937867  0.0653057 ]\n",
      "Epoch 4500, Cost: 0.35184444926918623, Beta: [-0.00086981 -0.42450354 -0.42851083  0.12083111 -0.10878411  0.33357113\n",
      " -0.37957293  0.20167961  0.64936559  0.06515634]\n",
      "Epoch 4600, Cost: 0.3518340894332639, Beta: [-0.00087097 -0.42643504 -0.43053001  0.12078991 -0.10891932  0.334642\n",
      " -0.3795162   0.20066802  0.64935198  0.065015  ]\n",
      "Epoch 4700, Cost: 0.3518246069051711, Beta: [-0.0008721  -0.42827106 -0.43244877  0.12075121 -0.10904205  0.33568444\n",
      " -0.37945605  0.19967069  0.64933814  0.0648812 ]\n",
      "Epoch 4800, Cost: 0.35181592137974926, Beta: [-0.00087319 -0.43001651 -0.43427228  0.12071487 -0.10915406  0.33669953\n",
      " -0.37939291  0.1986886   0.64932433  0.0647545 ]\n",
      "Epoch 4900, Cost: 0.3518079604293932, Beta: [-0.00087423 -0.43167604 -0.43600544  0.12068075 -0.10925691  0.33768827\n",
      " -0.37932718  0.19772255  0.64931077  0.06463449]\n",
      "Trained Ridge Regression Coefficients:  [-0.00087524 -0.43323865 -0.43763683  0.12064901 -0.10935101  0.33864205\n",
      " -0.3792599   0.19678261  0.64929776  0.06452188 -0.10860242  0.02072418\n",
      "  0.02788489  0.05527331]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Predict\n",
    "    y_pred = np.dot(X_train, beta)\n",
    "    # Calculate residuals\n",
    "    residuals = y_train - y_pred\n",
    "    # Calculate gradient\n",
    "    gradient = (-2 * (np.dot(X_train.T, residuals)) + (2 * LAMBDA * beta)) / X_train.shape[0]\n",
    "    # Update bias term\n",
    "    beta -= ALPHA * gradient\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Cost: {np.mean(residuals ** 2)}, Beta: {beta[:10]}\")\n",
    "\n",
    "print(\"Trained Ridge Regression Coefficients: \", beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.30691446  0.3300369  -0.02068932 ...  2.01136131 -0.73958894\n",
      " -0.20282866]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 56047.27104609, 244939.70509301, 204468.41774545, ...,\n",
       "       438952.47120035, 121512.56334535, 183450.84588621])"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict\n",
    "y_test_pred = X_test @ beta\n",
    "print(y_test_pred)\n",
    "y_test_pred_original = scaler_y.inverse_transform(y_test_pred.reshape(-1, 1)).flatten()\n",
    "y_test_pred_original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *2.1. MSE*\n",
    "\n",
    "$MSE = \\frac{1}{n} \\sum_{i=1}^n(y_i - \\hat{y_i})^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error:  0.36281572712363325\n"
     ]
    }
   ],
   "source": [
    "def mean_squared_error(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "print(\"Mean Squared Error: \", mean_squared_error(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. R-Squared\n",
    "\n",
    "$R^2 = 1 - \\frac{SS_{residuals}}{SS_{totals}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-Squared:  0.6323689736157119\n"
     ]
    }
   ],
   "source": [
    "def r_squared(y_true, y_pred):\n",
    "    ss_res = np.sum((y_true - y_pred) ** 2)\n",
    "    ss_total = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "    rs = 1 - (ss_res / ss_total)\n",
    "    return rs\n",
    "\n",
    "print(\"R-Squared: \", r_squared(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **C. PUTTING EVERYTHING TOGETHER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Annotated\n",
    "\n",
    "class RidgeRegressionModel:\n",
    "    def __init__(self,\n",
    "                 X: Annotated[np.ndarray, 'Features matrix'],\n",
    "                 y: Annotated[np.ndarray, 'Target values'],\n",
    "                 lambda_: Annotated[float, 'Regularization parameter'] = 1.0,\n",
    "                 epochs: Annotated[int, 'Number of epochs'] = 1000,\n",
    "                 learning_rate: Annotated[float, 'Learning rate'] = 0.01,\n",
    "                 ):\n",
    "        self.lambda_ = lambda_\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        _ , n_features = X.shape\n",
    "        self.w = np.zeros(n_features)\n",
    "        self.b = 0\n",
    "        for epoch in range(self.epochs):\n",
    "            self.update_weights(X, y)\n",
    "            if epoch % 100 == 0:\n",
    "                print(f'Epoch: {epoch}, Loss: {self.loss}')\n",
    "\n",
    "    def update_weights(self, X: np.ndarray, y: np.ndarray):\n",
    "        n_rows , _ = X.shape\n",
    "        y_pred = X @ self.w\n",
    "        res = y - y_pred\n",
    "        dW = ((-2 * (X.T @ (res)) + 2 * self.lambda_ * self.w) / n_rows)\n",
    "        dB = (-2 * np.sum(res)) / n_rows\n",
    "        self.w -= self.learning_rate * dW\n",
    "        self.b -= self.learning_rate * dB\n",
    "        self.loss = np.mean((y - y_pred) ** 2)\n",
    "        \n",
    "    def predict(self, X: np.ndarray):\n",
    "        return X @ self.w + self.b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
